{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 讀取套件\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['1', '2', '3', '4', '5', '6', '7', '8', '9', \"10\"]\n",
    "\n",
    "(x_train, y_train0), (x_test, y_test0) = mnist.load_data()\n",
    "\n",
    "x_train = x_train / x_train.max()\n",
    "x_test = x_test / x_test.max()\n",
    "\n",
    "y_train = to_categorical(y_train0, 10)\n",
    "y_test = to_categorical(y_test0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.random.randint(x_train.shape[0])\n",
    "x_sample = x_train[img]\n",
    "y_sample = y_train0[img].squeeze()\n",
    "\n",
    "plt.imshow(x_sample)\n",
    "plt.title(label[y_sample])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: (60000, 28, 28)\n",
      "Test Size: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print('Train Size:', x_train.shape)\n",
    "print('Test Size:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\huhu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "### 建構模型\n",
    "\n",
    "CNN_layer = [Conv2D(32, (3, 3), input_shape=(28, 28, 1), padding='same', activation='relu'),\n",
    "           MaxPool2D(),\n",
    "           Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "           MaxPool2D(),\n",
    "           Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "           MaxPool2D(),\n",
    "           Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "           GlobalAveragePooling2D()]\n",
    "FC_layer = [Dense(512, activation='relu'),\n",
    "              Dense(10, activation='softmax')]\n",
    "\n",
    "model = Sequential(CNN_layer + FC_layer)\n",
    "\n",
    "x_train=x_train.reshape(60000,28,28,1)\n",
    "x_test=x_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 524,554\n",
      "Trainable params: 524,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "                optimizer=Adam(),\n",
    "                metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 124s 2ms/sample - loss: 0.4220 - categorical_accuracy: 0.8628 - val_loss: 0.1019 - val_categorical_accuracy: 0.9670\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 125s 2ms/sample - loss: 0.0931 - categorical_accuracy: 0.9717 - val_loss: 0.0554 - val_categorical_accuracy: 0.9824\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 125s 2ms/sample - loss: 0.0605 - categorical_accuracy: 0.9813 - val_loss: 0.0367 - val_categorical_accuracy: 0.9876\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 126s 2ms/sample - loss: 0.0429 - categorical_accuracy: 0.9868 - val_loss: 0.0368 - val_categorical_accuracy: 0.9882\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 125s 2ms/sample - loss: 0.0360 - categorical_accuracy: 0.9887 - val_loss: 0.0336 - val_categorical_accuracy: 0.9889\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 125s 2ms/sample - loss: 0.0279 - categorical_accuracy: 0.9912 - val_loss: 0.0269 - val_categorical_accuracy: 0.9900\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 131s 2ms/sample - loss: 0.0251 - categorical_accuracy: 0.9918 - val_loss: 0.0222 - val_categorical_accuracy: 0.9917\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 125s 2ms/sample - loss: 0.0226 - categorical_accuracy: 0.9926 - val_loss: 0.0345 - val_categorical_accuracy: 0.9889\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 119s 2ms/sample - loss: 0.0176 - categorical_accuracy: 0.9943 - val_loss: 0.0268 - val_categorical_accuracy: 0.9917\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 100s 2ms/sample - loss: 0.0152 - categorical_accuracy: 0.9946 - val_loss: 0.0251 - val_categorical_accuracy: 0.9923\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0124 - categorical_accuracy: 0.9962 - val_loss: 0.0203 - val_categorical_accuracy: 0.9918\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 99s 2ms/sample - loss: 0.0155 - categorical_accuracy: 0.9949 - val_loss: 0.0270 - val_categorical_accuracy: 0.9902\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 100s 2ms/sample - loss: 0.0123 - categorical_accuracy: 0.9959 - val_loss: 0.0262 - val_categorical_accuracy: 0.9920\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 100s 2ms/sample - loss: 0.0111 - categorical_accuracy: 0.9962 - val_loss: 0.0291 - val_categorical_accuracy: 0.9902\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 100s 2ms/sample - loss: 0.0101 - categorical_accuracy: 0.9968 - val_loss: 0.0227 - val_categorical_accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "old = model.fit( x_train, y_train, batch_size=200, epochs=15, validation_data=(x_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 23s 385us/sample - loss: 0.0057 - categorical_accuracy: 0.9985\n",
      "10000/10000 [==============================] - 4s 383us/sample - loss: 0.0227 - categorical_accuracy: 0.9924\n",
      "訓練正確率: 99.84833598136902\n",
      "測試正確率: 99.23999905586243\n"
     ]
    }
   ],
   "source": [
    "score_train = model.evaluate(x_train, y_train)\n",
    "score_test = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'訓練正確率: {score_train[1]*100}')\n",
    "print(f'測試正確率: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 讀入新資料\n",
    "\n",
    "(X_train, Y_train0), (X_test, Y_test0) = fashion_mnist.load_data()\n",
    "\n",
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()\n",
    "\n",
    "Y_train = to_categorical(Y_train0, 100)\n",
    "Y_test = to_categorical(Y_test0, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.random.randint(X_train.shape[0])\n",
    "X_sample = X_train[img]\n",
    "Y_sample = Y_train0[img].squeeze()\n",
    "\n",
    "plt.imshow(X_sample)\n",
    "plt.title(label[y_sample])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in CNN_layer:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_layer_new = [Dense(128, activation='relu'),\n",
    "              Dense(64, activation='relu'),\n",
    "              Dense(100, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               6500      \n",
      "=================================================================\n",
      "Total params: 435,492\n",
      "Trainable params: 47,652\n",
      "Non-trainable params: 387,840\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trans = Sequential(CNN_layer+FC_layer_new)\n",
    "model_trans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trans.compile(loss='categorical_crossentropy', \n",
    "                optimizer=Adam(),\n",
    "                metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 22s 362us/sample - loss: 0.9698 - categorical_accuracy: 0.6797 - val_loss: 0.6439 - val_categorical_accuracy: 0.7564\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 23s 386us/sample - loss: 0.5876 - categorical_accuracy: 0.7833 - val_loss: 0.5814 - val_categorical_accuracy: 0.7779\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 24s 401us/sample - loss: 0.5327 - categorical_accuracy: 0.8051 - val_loss: 0.5399 - val_categorical_accuracy: 0.7965\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 24s 404us/sample - loss: 0.5003 - categorical_accuracy: 0.8178 - val_loss: 0.5187 - val_categorical_accuracy: 0.8091\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 24s 404us/sample - loss: 0.4740 - categorical_accuracy: 0.8292 - val_loss: 0.4907 - val_categorical_accuracy: 0.8180\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 25s 414us/sample - loss: 0.4577 - categorical_accuracy: 0.8337 - val_loss: 0.4768 - val_categorical_accuracy: 0.8264\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 25s 421us/sample - loss: 0.4411 - categorical_accuracy: 0.8400 - val_loss: 0.4786 - val_categorical_accuracy: 0.8227\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 25s 416us/sample - loss: 0.4296 - categorical_accuracy: 0.8440 - val_loss: 0.4662 - val_categorical_accuracy: 0.8269\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 25s 410us/sample - loss: 0.4155 - categorical_accuracy: 0.8492 - val_loss: 0.4549 - val_categorical_accuracy: 0.8329\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 25s 413us/sample - loss: 0.4094 - categorical_accuracy: 0.8515 - val_loss: 0.4447 - val_categorical_accuracy: 0.8383\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 25s 421us/sample - loss: 0.4003 - categorical_accuracy: 0.8541 - val_loss: 0.4398 - val_categorical_accuracy: 0.8391\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 25s 419us/sample - loss: 0.3910 - categorical_accuracy: 0.8577 - val_loss: 0.4347 - val_categorical_accuracy: 0.8417\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 25s 415us/sample - loss: 0.3846 - categorical_accuracy: 0.8587 - val_loss: 0.4380 - val_categorical_accuracy: 0.8409\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 25s 418us/sample - loss: 0.3782 - categorical_accuracy: 0.8611 - val_loss: 0.4182 - val_categorical_accuracy: 0.8492\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 25s 416us/sample - loss: 0.3724 - categorical_accuracy: 0.8629 - val_loss: 0.4108 - val_categorical_accuracy: 0.8493\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 25s 423us/sample - loss: 0.3681 - categorical_accuracy: 0.8644 - val_loss: 0.4253 - val_categorical_accuracy: 0.8443\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 25s 416us/sample - loss: 0.3631 - categorical_accuracy: 0.8669 - val_loss: 0.4111 - val_categorical_accuracy: 0.8503\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 25s 416us/sample - loss: 0.3595 - categorical_accuracy: 0.8677 - val_loss: 0.4153 - val_categorical_accuracy: 0.8493\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 25s 417us/sample - loss: 0.3542 - categorical_accuracy: 0.8686 - val_loss: 0.4103 - val_categorical_accuracy: 0.8487\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 25s 419us/sample - loss: 0.3506 - categorical_accuracy: 0.8704 - val_loss: 0.4038 - val_categorical_accuracy: 0.8503\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.reshape(60000,28,28,1)\n",
    "X_test=X_test.reshape(10000,28,28,1)\n",
    "\n",
    "new = model_trans.fit( X_train, Y_train, batch_size=128, epochs=20, validation_data=(X_test, Y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 23s 389us/sample - loss: 0.3443 - categorical_accuracy: 0.8748\n",
      "10000/10000 [==============================] - 4s 382us/sample - loss: 0.4038 - categorical_accuracy: 0.8503\n",
      "訓練正確率: 87.48000264167786\n",
      "測試正確率: 85.03000140190125\n"
     ]
    }
   ],
   "source": [
    "trans_score_train = model_trans.evaluate(X_train, Y_train)\n",
    "trans_score_test = model_trans.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f'訓練正確率: {trans_score_train[1]*100}')\n",
    "print(f'測試正確率: {trans_score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
